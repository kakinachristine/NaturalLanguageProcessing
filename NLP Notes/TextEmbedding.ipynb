{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Word To Vector (word2vec):\n",
    "\n",
    "- Continuous bag of words(cbow\n",
    "- Skip gram"
   ],
   "id": "7febecaef2303884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.489903Z",
     "start_time": "2025-07-24T11:00:43.475457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Import Libraries\n",
    "from gensim.models import Word2Vec"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.541621Z",
     "start_time": "2025-07-24T11:00:43.509141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Four sample tokenized sentences (documents)\n",
    "tokenizedSentences = [\n",
    "    [\"natural\", \"language\", \"processing\", \"is\", \"fun\"],\n",
    "    [\"gensim\", \"makes\", \"topic\", \"modeling\", \"easy\"],\n",
    "    [\"we\", \"can\", \"train\", \"word2vec\", \"on\", \"our\", \"own\", \"corpus\"],\n",
    "    [\"this\", \"example\", \"uses\", \"simple\", \"tokenized\", \"sentences\"]\n",
    "]\n",
    "\n",
    "# Building CBOW Word2Vec model(sg parameter is set to 0)\n",
    "cbowModel= Word2Vec(tokenizedSentences,vector_size=100,window=2,sg=1, min_count=1)\n",
    "\n",
    "# Building skip-gram Word2Vec model\n",
    "skipGramModel = Word2Vec(tokenizedSentences,vector_size=10,window=2,sg=1,min_count=1)"
   ],
   "id": "da41e7a9a8bd9ca9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.562446Z",
     "start_time": "2025-07-24T11:00:43.554318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Example of word for similarity comparison\n",
    "targetWord = \"natural\"\n",
    "\n",
    "# Get similar words using CBOW model\n",
    "similarWordsCbow = cbowModel.wv.most_similar(targetWord, topn=2)\n",
    "print(\"CBOW Similar Words:\", similarWordsCbow)\n",
    "\n",
    "# Get similar words using Skip-gram model\n",
    "similarWordSkipgram = skipGramModel.wv.most_similar(targetWord, topn=5)\n",
    "print(\"Skip-gram Similar Words:\", similarWordSkipgram)\n"
   ],
   "id": "5078df970b05ecaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Similar Words: [('word2vec', 0.17132115364074707), ('uses', 0.1566610485315323)]\n",
      "Skip-gram Similar Words: [('word2vec', 0.6313896775245667), ('tokenized', 0.41193631291389465), ('on', 0.35338476300239563), ('gensim', 0.3428654074668884), ('train', 0.2825911343097687)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.587908Z",
     "start_time": "2025-07-24T11:00:43.579606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Words to compare\n",
    "word1 = \"natural\"\n",
    "word2 = \"language\"\n",
    "\n",
    "# Get vectors from CBOW model\n",
    "vector1 = cbowModel.wv[word1]\n",
    "vector2 = cbowModel.wv[word2]\n",
    "\n",
    "# Method 1: Using gensim’s internal cosine similarity\n",
    "cosine_similarity = cbowModel.wv.similarity(word1, word2)\n",
    "\n",
    "# Method 2 (manual with numpy, for learning)\n",
    "import numpy as np\n",
    "\n",
    "manual_cosine = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}' (CBOW): {cosine_similarity:.4f}\")\n",
    "print(f\"Manual cosine similarity: {manual_cosine:.4f}\")\n"
   ],
   "id": "44fe86f6b4e60bf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'natural' and 'language' (CBOW): -0.2156\n",
      "Manual cosine similarity: -0.2156\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.609433Z",
     "start_time": "2025-07-24T11:00:43.605975Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7d43e3cfc4ad4586",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.635255Z",
     "start_time": "2025-07-24T11:00:43.630693Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a59fc5c2ee4e92f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:00:43.664009Z",
     "start_time": "2025-07-24T11:00:43.650594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: build a Dictionary and corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a mapping (word -> id)\n",
    "dictionary = Dictionary(tokenizedSentences)\n",
    "\n",
    "# Convert each sentence into bag‑of‑words (list of (id, count))\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenizedSentences]\n",
    "\n",
    "# (Optional) Fit a TF-IDF model on that corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "print(\"Dictionary token-to-id mapping:\")\n",
    "print(dictionary.token2id)\n",
    "print(\"\\nBoW corpus:\")\n",
    "print(corpus)\n",
    "print(\"\\nTF-IDF corpus:\")\n",
    "print([list(doc) for doc in corpus_tfidf])"
   ],
   "id": "c6dd86646c6f90af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary token-to-id mapping:\n",
      "{'fun': 0, 'is': 1, 'language': 2, 'natural': 3, 'processing': 4, 'easy': 5, 'gensim': 6, 'makes': 7, 'modeling': 8, 'topic': 9, 'can': 10, 'corpus': 11, 'on': 12, 'our': 13, 'own': 14, 'train': 15, 'we': 16, 'word2vec': 17, 'example': 18, 'sentences': 19, 'simple': 20, 'this': 21, 'tokenized': 22, 'uses': 23}\n",
      "\n",
      "BoW corpus:\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]]\n",
      "\n",
      "TF-IDF corpus:\n",
      "[[(0, 0.4472135954999579), (1, 0.4472135954999579), (2, 0.4472135954999579), (3, 0.4472135954999579), (4, 0.4472135954999579)], [(5, 0.4472135954999579), (6, 0.4472135954999579), (7, 0.4472135954999579), (8, 0.4472135954999579), (9, 0.4472135954999579)], [(10, 0.35355339059327373), (11, 0.35355339059327373), (12, 0.35355339059327373), (13, 0.35355339059327373), (14, 0.35355339059327373), (15, 0.35355339059327373), (16, 0.35355339059327373), (17, 0.35355339059327373)], [(18, 0.4082482904638631), (19, 0.4082482904638631), (20, 0.4082482904638631), (21, 0.4082482904638631), (22, 0.4082482904638631), (23, 0.4082482904638631)]]\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
