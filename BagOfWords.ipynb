{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Natural Language Preprocessing\n",
    "#### Preprocessing in NLP\n",
    "###### -Tokenization: converts a single sentence/ text into single words\n",
    "###### -Lowercasing: convert all words to lowercase\n",
    "###### -Base form of the word: Convert words to single base form,finds unique word in words that share a common meaning(jump,jumped,jumping,jumps to jump) uses: stemming,lemmatization\n",
    "###### -stopword removal: removing common words that are for english like this, she , the e.t.c. This words are not important hence removal\n",
    "###### -Punctuation removal\n",
    "###### -Remove contraction e.g. from can't to cannot"
   ],
   "id": "778fbcec9680e8c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c203a55aa9711eae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ§  Bag of Words (BoW)",
   "id": "f80190ca7bc126cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 29,
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "data1 = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Bag of words is a simple technique\",\n",
    "    \"Machine learning is fun and powerful\",\n",
    "    \"Text classification uses word counts\",\n",
    "    \"I enjoy learning about data science\",\n",
    "    \"This model converts text to vectors\",\n",
    "    \"NLP tasks include sentiment analysis\",\n",
    "    \"Text data needs preprocessing\",\n",
    "    \"Feature extraction is important in NLP\",\n",
    "    \"We use vectorization to represent text\"\n",
    "]\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 30,
   "source": [
    "#Import Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ],
   "id": "3f698ae3ed9f89a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 53 stored elements and shape (10, 44)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31,
   "source": [
    "# vectorize the data into a numeric matrix\n",
    "cv = CountVectorizer()\n",
    "newData=cv.fit_transform(data1)\n",
    "newData\n",
    "# Output: metrix of  10 rows and 44 columns with 53 non-zero word counts\n",
    "# Sparse matrix :each word in sentence has to be represented in the 44 columns(representation of unique words)\n",
    "# Representation is in integer (64)\n",
    "# The count/entry depends on the occurrence of a word in each sentence(row)\n",
    "# text case sensitive to convert all text to text\n"
   ],
   "id": "1102228d130ec7e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32,
   "source": "newData.toarray()",
   "id": "b731adb15c123124"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 44)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33,
   "source": "newData.shape",
   "id": "49d86c62124edbbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34,
   "source": [
    "#Dimensuion of the matrix\n",
    "newData.ndim"
   ],
   "id": "16550a25b97a62e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'analysis', 'and', 'bag', 'classification', 'converts',\n",
       "       'counts', 'data', 'enjoy', 'extraction', 'feature', 'fun',\n",
       "       'important', 'in', 'include', 'is', 'language', 'learning', 'love',\n",
       "       'machine', 'model', 'natural', 'needs', 'nlp', 'of', 'powerful',\n",
       "       'preprocessing', 'processing', 'represent', 'science', 'sentiment',\n",
       "       'simple', 'tasks', 'technique', 'text', 'this', 'to', 'use',\n",
       "       'uses', 'vectorization', 'vectors', 'we', 'word', 'words'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35,
   "source": [
    "#Get the unique words\n",
    "cv.get_feature_names_out()"
   ],
   "id": "445e38a73d75e8c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   about  analysis  and  bag  classification  converts  counts  data  enjoy  \\\n",
       "0      0         0    0    0               0         0       0     0      0   \n",
       "1      0         0    0    1               0         0       0     0      0   \n",
       "2      0         0    1    0               0         0       0     0      0   \n",
       "3      0         0    0    0               1         0       1     0      0   \n",
       "4      1         0    0    0               0         0       0     1      1   \n",
       "5      0         0    0    0               0         1       0     0      0   \n",
       "6      0         1    0    0               0         0       0     0      0   \n",
       "7      0         0    0    0               0         0       0     1      0   \n",
       "8      0         0    0    0               0         0       0     0      0   \n",
       "9      0         0    0    0               0         0       0     0      0   \n",
       "\n",
       "   extraction  ...  text  this  to  use  uses  vectorization  vectors  we  \\\n",
       "0           0  ...     0     0   0    0     0              0        0   0   \n",
       "1           0  ...     0     0   0    0     0              0        0   0   \n",
       "2           0  ...     0     0   0    0     0              0        0   0   \n",
       "3           0  ...     1     0   0    0     1              0        0   0   \n",
       "4           0  ...     0     0   0    0     0              0        0   0   \n",
       "5           0  ...     1     1   1    0     0              0        1   0   \n",
       "6           0  ...     0     0   0    0     0              0        0   0   \n",
       "7           0  ...     1     0   0    0     0              0        0   0   \n",
       "8           1  ...     0     0   0    0     0              0        0   0   \n",
       "9           0  ...     1     0   1    1     0              1        0   1   \n",
       "\n",
       "   word  words  \n",
       "0     0      0  \n",
       "1     0      1  \n",
       "2     0      0  \n",
       "3     1      0  \n",
       "4     0      0  \n",
       "5     0      0  \n",
       "6     0      0  \n",
       "7     0      0  \n",
       "8     0      0  \n",
       "9     0      0  \n",
       "\n",
       "[10 rows x 44 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>analysis</th>\n",
       "      <th>and</th>\n",
       "      <th>bag</th>\n",
       "      <th>classification</th>\n",
       "      <th>converts</th>\n",
       "      <th>counts</th>\n",
       "      <th>data</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>extraction</th>\n",
       "      <th>...</th>\n",
       "      <th>text</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>use</th>\n",
       "      <th>uses</th>\n",
       "      <th>vectorization</th>\n",
       "      <th>vectors</th>\n",
       "      <th>we</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 44 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36,
   "source": [
    "# Initialize dataframe from our newData array and out columns from our data unique words\n",
    "df= pd.DataFrame(data=newData.toarray(),columns=cv.get_feature_names_out())\n",
    "df"
   ],
   "id": "4185236d23563b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TEXT PREPROCESSING",
   "id": "148e91e1fe89e5a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:06.661204Z",
     "start_time": "2025-07-24T07:15:06.656891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "fd53aba00f205a12",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. STOPWORDS",
   "id": "987a82b84b184d50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:06.730355Z",
     "start_time": "2025-07-24T07:15:06.723373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ],
   "id": "8115f3112fa94895",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:06.957295Z",
     "start_time": "2025-07-24T07:15:06.952217Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(stopwords.words('english')))",
   "id": "967f0375e471fd75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:07.024799Z",
     "start_time": "2025-07-24T07:15:07.019730Z"
    }
   },
   "cell_type": "code",
   "source": "stopWordList=stopwords.words('english')",
   "id": "6e8e580a78dd3d19",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. TOKENIZATION",
   "id": "12552c8e28eb14a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:07.055267Z",
     "start_time": "2025-07-24T07:15:07.050495Z"
    }
   },
   "cell_type": "code",
   "source": "paragraph = \"\"\"Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses on the interaction between computers and humans through language. It allows machines to read, understand, and derive meaning from human languages. NLP is used in applications like chatbots, translation, sentiment analysis, and more.\"\"\"\n",
   "id": "6a61f68f8ef56112",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:07.090131Z",
     "start_time": "2025-07-24T07:15:07.083419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ],
   "id": "7c976dffbf940134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'language', '.', 'It', 'allows', 'machines', 'to', 'read', ',', 'understand', ',', 'and', 'derive', 'meaning', 'from', 'human', 'languages', '.', 'NLP', 'is', 'used', 'in', 'applications', 'like', 'chatbots', ',', 'translation', ',', 'sentiment', 'analysis', ',', 'and', 'more', '.']\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:17.084924Z",
     "start_time": "2025-07-24T07:15:07.141510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ],
   "id": "e718ebeda77ca9b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:17.116920Z",
     "start_time": "2025-07-24T07:15:17.112419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ],
   "id": "144e5af123f591c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses on the interaction between computers and humans through language.\n",
      "It allows machines to read, understand, and derive meaning from human languages.\n",
      "NLP is used in applications like chatbots, translation, sentiment analysis, and more.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:17.177843Z",
     "start_time": "2025-07-24T07:15:17.153034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for word in stopwords.words('english'):\n",
    "#     print(word)\n",
    "#\n",
    "# print(\"No of stopwords: \",len(stopwords.words('english')))\n",
    "\n",
    "\n",
    "# Removing the stop words\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i]=' '.join(words)\n",
    "# Check specific sentence after stopword removal\n",
    "sentences[2:5]"
   ],
   "id": "18ec3aed73a916f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP used applications like chatbots , translation , sentiment analysis , .']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:17.204740Z",
     "start_time": "2025-07-24T07:15:17.202319Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "90acbe6a299e643c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. STEMMING AND LEMMATIZATION",
   "id": "f7f7de26834cdd54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:15:17.242873Z",
     "start_time": "2025-07-24T07:15:17.234981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words =[stemmer.stem(word)for word in words]\n",
    "    sentences[i]=' '.join(words)\n",
    "sentences"
   ],
   "id": "ab3083531937b2f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus interact comput human languag .',\n",
       " 'it allow machin read , understand , deriv mean human languag .',\n",
       " 'nlp use applic like chatbot , translat , sentiment analysi , .']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:19:16.528342Z",
     "start_time": "2025-07-24T07:19:03.675823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    sentences[i] = ' '.join(words)\n",
    "\n",
    "print(sentences)\n"
   ],
   "id": "850e702853eb3e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process ( nlp ) field artifici intellig focus interact comput human languag .', 'it allow machin read , understand , deriv mean human languag .', 'nlp use applic like chatbot , translat , sentiment analysi , .']\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Part of Speech Tagging",
   "id": "7fea2d1814aadf97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:16:13.015108Z",
     "start_time": "2025-07-24T07:16:11.130118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How to get part of speech in  a particular sentence(parts of speech: noun,adjectives, adverbs,verbs,proposition,determiner...)\n",
    "# POS tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "taggedWords = nltk.pos_tag(words)\n",
    "\n",
    "# Tagged word paragraph\n",
    "wordTags=[]\n",
    "for tw in taggedWords:\n",
    "    wordTags.append(tw[0]+ \"_\"+tw[1])\n",
    "\n",
    "taggedParagraph = nltk.pos_tag(wordTags)\n",
    "print(taggedParagraph)"
   ],
   "id": "84357832ae1a326c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural_JJ', 'NNP'), ('Language_NNP', 'NNP'), ('Processing_NNP', 'NNP'), ('(_(', 'NNP'), ('NLP_NNP', 'NNP'), (')_)', 'NNP'), ('is_VBZ', 'NN'), ('a_DT', 'NN'), ('field_NN', 'NN'), ('of_IN', 'NN'), ('Artificial_NNP', 'NNP'), ('Intelligence_NNP', 'NNP'), ('that_WDT', 'NN'), ('focuses_VBZ', 'NN'), ('on_IN', 'NN'), ('the_DT', 'NN'), ('interaction_NN', 'NN'), ('between_IN', 'NN'), ('computers_NNS', 'NN'), ('and_CC', 'NN'), ('humans_NNS', 'NN'), ('through_IN', 'NN'), ('language_NN', 'NN'), ('._.', 'NNP'), ('It_PRP', 'NNP'), ('allows_VBZ', 'VBZ'), ('machines_NNS', 'FW'), ('to_TO', 'NN'), ('read_VB', 'NN'), (',_,', 'NNP'), ('understand_VB', 'JJ'), (',_,', 'NNP'), ('and_CC', 'NN'), ('derive_JJ', 'NN'), ('meaning_NN', 'NN'), ('from_IN', 'NN'), ('human_JJ', 'NN'), ('languages_NNS', 'NN'), ('._.', 'NNP'), ('NLP_NNP', 'NNP'), ('is_VBZ', 'NN'), ('used_VBN', 'JJ'), ('in_IN', 'NN'), ('applications_NNS', 'NN'), ('like_IN', 'NN'), ('chatbots_NNS', 'NN'), (',_,', 'NNP'), ('translation_NN', 'NN'), (',_,', 'NNP'), ('sentiment_NN', 'NN'), ('analysis_NN', 'NN'), (',_,', 'NNP'), ('and_CC', 'NN'), ('more_JJR', 'NN'), ('._.', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c1e343aaafb86c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6. Working with Regular Expression(re) Library for  String and Pattern Functions",
   "id": "bc638c74b666f59d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ðŸ§¾ Explanation:\n",
    "re.sub(r\"avengers\", \"Justice League\", ..., flags=re.I):\n",
    "\n",
    "Matches \"avengers\" regardless of casing (e.g., \"Avengers\", \"AVENGERS\", etc.)\n",
    "\n",
    "Replaces it with \"Justice League\"\n",
    "\n",
    "re.sub(r\"[a-z]\", \"2\", ..., count=1, flags=re.I):\n",
    "\n",
    "Replaces only the first alphabetic character, case-insensitive, with \"2\"\n",
    "\n",
    "So \"I\" (capital i) is matched and replaced."
   ],
   "id": "11cc6997eeafdbcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:31:34.947287Z",
     "start_time": "2025-07-24T07:31:34.941755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "pattern1 = \"00I love avengers\"  # I love justice league\n",
    "\n",
    "# Replace 'avengers' with 'Justice League' (case-insensitive)\n",
    "print(re.sub(r\"avengers\", \"Justice League\", pattern1, flags=re.I))\n",
    "\n",
    "# Replace the alphabetical letter (case-insensitive) with '2'\n",
    "print(re.sub(r\"[a-z]\", \"2\", pattern1, count=1, flags=re.I))\n",
    "\n"
   ],
   "id": "bc38a3a4f8d87c44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00I love Justice League\n",
      "002 love avengers\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:35:39.707062Z",
     "start_time": "2025-07-24T07:35:39.702483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pattern1 = \"00I love avengers avengers\"\n",
    "print(re.sub(r\"avengers\", \"Justice League\", pattern1, flags=re.I))"
   ],
   "id": "f2701cafba26c399",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00I love Justice League Justice League\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:38:47.635353Z",
     "start_time": "2025-07-24T07:38:47.630307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Removing punctuation marks\n",
    "textStr= \"George is the best. Doing so for! all good?\"\n",
    "res= re.sub(r\"[^\\w\\s]\", \"\",textStr )\n",
    "print(res)"
   ],
   "id": "854c4625e3605681",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George is the best Doing so for all good\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T07:52:44.930001Z",
     "start_time": "2025-07-24T07:52:44.922220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "sampleText = \"\"\"\n",
    "3.14 I had a slice of apple pie while\n",
    "\n",
    " reading an email from alice@example.com.\n",
    "Later,          I replied to bob123@mail.co.uk and ordered more pie from pieheaven@food.com.\n",
    "Email me at test.user@domain.org     if you want pie 234 recipes. 1\n",
    "\"\"\"\n",
    "# Find all words (alphanumeric sequences between word boundaries)\n",
    "words = re.findall(r'\\b\\w+\\b', sampleText)\n",
    "\n",
    "# Find all email addresses\n",
    "emails = re.findall(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', sampleText)\n",
    "\n",
    "# Find all occurrences of the word 'pie' (case-insensitive)\n",
    "pies = re.findall(r'\\bpie\\b', sampleText, flags=re.I)\n",
    "\n",
    "# Find numbers\n",
    "numbers = re.findall(r'\\b\\d+\\d+', sampleText) #3.14 is not included\n",
    "\n",
    "# Remove whitespaces\n",
    "noWhiteSpaces = re.sub(r'\\s+', '', sampleText)\n",
    "\n",
    "\n",
    "# Split on ., !, or ? (and keep the delimiters out)\n",
    "parts = re.split(r'[.?!]', sampleText)\n",
    "\n",
    "# Strip whitespace and drop empty parts\n",
    "sampleSentence = [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "\n",
    "# Print results\n",
    "for i, sent in enumerate(sampleSentence, 1):\n",
    "    print(f\"Sentence {i}: {sent}\")\n",
    "print(\"Words:\", words)\n",
    "print(\"Emails:\", emails)\n",
    "print(\"Occurrences of 'pie':\", pies)\n",
    "print(\"numbers:\", numbers)\n",
    "print(\"No white spaces:\",noWhiteSpaces)"
   ],
   "id": "e703448a902ddbb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 3\n",
      "Sentence 2: 14 I had a slice of apple pie while\n",
      "\n",
      " reading an email from alice@example\n",
      "Sentence 3: com\n",
      "Sentence 4: Later,          I replied to bob123@mail\n",
      "Sentence 5: co\n",
      "Sentence 6: uk and ordered more pie from pieheaven@food\n",
      "Sentence 7: com\n",
      "Sentence 8: Email me at test\n",
      "Sentence 9: user@domain\n",
      "Sentence 10: org     if you want pie 234 recipes\n",
      "Sentence 11: 1\n",
      "Words: ['3', '14', 'I', 'had', 'a', 'slice', 'of', 'apple', 'pie', 'while', 'reading', 'an', 'email', 'from', 'alice', 'example', 'com', 'Later', 'I', 'replied', 'to', 'bob123', 'mail', 'co', 'uk', 'and', 'ordered', 'more', 'pie', 'from', 'pieheaven', 'food', 'com', 'Email', 'me', 'at', 'test', 'user', 'domain', 'org', 'if', 'you', 'want', 'pie', '234', 'recipes', '1']\n",
      "Emails: ['alice@example.com', 'bob123@mail.co.uk', 'pieheaven@food.com', 'test.user@domain.org']\n",
      "Occurrences of 'pie': ['pie', 'pie', 'pie']\n",
      "numbers: ['14', '234']\n",
      "No white spaces: 3.14Ihadasliceofapplepiewhilereadinganemailfromalice@example.com.Later,Irepliedtobob123@mail.co.ukandorderedmorepiefrompieheaven@food.com.Emailmeattest.user@domain.orgifyouwantpie234recipes.1\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6206c383505a3fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
