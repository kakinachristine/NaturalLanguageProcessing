{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Advanced Vectorization\n",
    "\n",
    " - Bag of words(Simple frequency,Relative frequency,Text Frequency and Inverse document frequency)\n",
    " - Words may be unigrams/N-grams with N being 2,3,4.......\n"
   ],
   "id": "74d5eb3b5f3f536b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What Are N‑grams?\n",
    "\n",
    "An **n‑gram** is a contiguous sequence of *n* items (tokens) from a given sample of text or speech.\n",
    "- **Items** can be **words** or **characters**.\n",
    "- **n** can be any positive integer (1, 2, 3, …).\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use N‑grams?\n",
    "\n",
    "- **Language modeling** (predict next word)\n",
    "- **Text classification** (features in Naïve Bayes, SVM, etc.)\n",
    "- **Plagiarism detection** (match overlapping sequences)\n",
    "- **Spelling correction** (detect likely sequences)\n"
   ],
   "id": "ee31f073d6f9d020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# N‑grams example\n",
    "\n",
    "Imagine the sentence:\n",
    "\n",
    "> **“I love chocolate cake.”**\n",
    "\n",
    "---\n",
    "\n",
    "## 1‑grams (Unigrams)\n",
    "\n",
    "Each single word by itself:\n",
    "\n",
    "- I\n",
    "- love\n",
    "- chocolate\n",
    "- cake\n",
    "\n",
    "---\n",
    "\n",
    "## 2‑grams (Bigrams)\n",
    "\n",
    "Every pair of words in a row:\n",
    "\n",
    "- I love\n",
    "- love chocolate\n",
    "- chocolate cake\n",
    "\n",
    "---\n",
    "\n",
    "## 3‑grams (Trigrams)\n",
    "\n",
    "Every three words in a row:\n",
    "\n",
    "- I love chocolate\n",
    "- love chocolate cake\n",
    "\n",
    "---\n",
    "\n",
    "*An n‑gram is simply “n” things stuck together exactly as they appeared in the text, sliding one step at a time.This helps computers guess what comes next, or spot patterns in writing..*\n"
   ],
   "id": "f19743c7e9b77a0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T08:48:56.625503Z",
     "start_time": "2025-07-24T08:48:56.619744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "# Ensure required data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentence\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate and display n-grams\n",
    "for n in (1, 2, 3):\n",
    "    ng = list(ngrams(tokens, n))\n",
    "    print(f\"{n}-grams ({len(ng)}):\")\n",
    "    for gram in ng:\n",
    "        print(\" \", gram)\n",
    "    print()\n"
   ],
   "id": "edac5829c8f96287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams (9):\n",
      "  ('The',)\n",
      "  ('quick',)\n",
      "  ('brown',)\n",
      "  ('fox',)\n",
      "  ('jumps',)\n",
      "  ('over',)\n",
      "  ('the',)\n",
      "  ('lazy',)\n",
      "  ('dog',)\n",
      "\n",
      "2-grams (8):\n",
      "  ('The', 'quick')\n",
      "  ('quick', 'brown')\n",
      "  ('brown', 'fox')\n",
      "  ('fox', 'jumps')\n",
      "  ('jumps', 'over')\n",
      "  ('over', 'the')\n",
      "  ('the', 'lazy')\n",
      "  ('lazy', 'dog')\n",
      "\n",
      "3-grams (7):\n",
      "  ('The', 'quick', 'brown')\n",
      "  ('quick', 'brown', 'fox')\n",
      "  ('brown', 'fox', 'jumps')\n",
      "  ('fox', 'jumps', 'over')\n",
      "  ('jumps', 'over', 'the')\n",
      "  ('over', 'the', 'lazy')\n",
      "  ('the', 'lazy', 'dog')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T08:48:56.667922Z",
     "start_time": "2025-07-24T08:48:56.664824Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b88c87fff6ac0c3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building a Bag‑of‑Words from N‑grams\n",
    "\n",
    "Below are two ways to turn your n‑grams into a BoW representation in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Manual BoW with `collections.Counter`\n",
    "\n"
   ],
   "id": "71766dd1ab7deab5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T08:48:56.696873Z",
     "start_time": "2025-07-24T08:48:56.687099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate all 1‑, 2‑, and 3‑grams as space‑joined strings\n",
    "all_ngrams = []\n",
    "for n in (1, 2, 3):\n",
    "    all_ngrams += [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "# Build BoW counts\n",
    "bow = Counter(all_ngrams)\n",
    "\n",
    "# Display result\n",
    "for gram, count in bow.items():\n",
    "    print(f\"{gram!r}: {count}\")"
   ],
   "id": "bbc26fc76332b890",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The': 1\n",
      "'quick': 1\n",
      "'brown': 1\n",
      "'fox': 1\n",
      "'jumps': 1\n",
      "'over': 1\n",
      "'the': 1\n",
      "'lazy': 1\n",
      "'dog': 1\n",
      "'The quick': 1\n",
      "'quick brown': 1\n",
      "'brown fox': 1\n",
      "'fox jumps': 1\n",
      "'jumps over': 1\n",
      "'over the': 1\n",
      "'the lazy': 1\n",
      "'lazy dog': 1\n",
      "'The quick brown': 1\n",
      "'quick brown fox': 1\n",
      "'brown fox jumps': 1\n",
      "'fox jumps over': 1\n",
      "'jumps over the': 1\n",
      "'over the lazy': 1\n",
      "'the lazy dog': 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChristineKakina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Term Frequency (TF) and TF‑IDF for N‑grams\n",
    "\n",
    "You can compute **Term Frequency (TF)** and **TF‑IDF** over n‑grams very easily with scikit‑learn. Below are two examples using the same sample corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Term Frequency (TF) with CountVectorizer"
   ],
   "id": "505a2acdc198ba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T08:48:56.745743Z",
     "start_time": "2025-07-24T08:48:56.734372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love chocolate cake\",\n",
    "    \"Chocolate cake is delicious\",\n",
    "    \"Do you love cake too\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer for unigrams and bigrams\n",
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit to corpus and transform into document-term matrix\n",
    "X_counts = cv.fit_transform(corpus)\n",
    "\n",
    "# Raw counts\n",
    "feature_names = cv.get_feature_names_out()\n",
    "print(\"Features (n‑grams):\\n\", feature_names, \"\\n\")\n",
    "\n",
    "print(\"Raw counts matrix:\\n\", X_counts.toarray(), \"\\n\")\n",
    "\n",
    "# Normalize to get term frequency per document\n",
    "tf = X_counts.toarray().astype(float)\n",
    "tf = tf / tf.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"Term Frequency (TF) matrix:\\n\", tf)"
   ],
   "id": "c7d21d70fa3f424",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (n‑grams):\n",
      " ['cake' 'cake is' 'cake too' 'chocolate' 'chocolate cake' 'delicious' 'do'\n",
      " 'do you' 'is' 'is delicious' 'love' 'love cake' 'love chocolate' 'too'\n",
      " 'you' 'you love'] \n",
      "\n",
      "Raw counts matrix:\n",
      " [[1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1]] \n",
      "\n",
      "Term Frequency (TF) matrix:\n",
      " [[0.2        0.         0.         0.2        0.2        0.\n",
      "  0.         0.         0.         0.         0.2        0.\n",
      "  0.2        0.         0.         0.        ]\n",
      " [0.14285714 0.14285714 0.         0.14285714 0.14285714 0.14285714\n",
      "  0.         0.         0.14285714 0.14285714 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.11111111 0.         0.11111111 0.         0.         0.\n",
      "  0.11111111 0.11111111 0.         0.         0.11111111 0.11111111\n",
      "  0.         0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
